{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from preprocessor.preprocessor import tokenize\n",
    "\n",
    "import dataclasses\n",
    "import note_seq\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Any, Callable, MutableMapping, Optional, Sequence, Tuple, TypeVar, MutableSet\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from preprocessor import vocabularies\n",
    "from preprocessor.event_codec import Codec, Event\n",
    "from preprocessor.preprocessor import *\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/import/c4dm-datasets/maestro-v3.0.0/2013/ORIG-MIDI_02_7_6_13_Group__MID--AUDIO_06_R1_2013_wav--4.midi\"\n",
    "codec = vocabularies.build_codec(100, 256/50)\n",
    "ns = read_midi(filename)\n",
    "ns = note_seq.apply_sustain_control_changes(ns)\n",
    "num_frames = np.ceil(ns.total_time * 50)\n",
    "frame_times = torch.arange(num_frames) / 50 \n",
    "times, values = note_sequence_to_onsets_and_offsets_and_programs(ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.array(times)\n",
    "d_idx = np.nonzero(np.logical_and(d > 59 * 5.12, d < 60 * 5.12))\n",
    "e = np.array(values)[d_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_time(times, frame_length):\n",
    "    steps = np.round(np.array(times) / frame_length)\n",
    "    return steps.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_time = quantize_time(times, 0.01)\n",
    "time_stamps = np.unique(quantized_time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = {}\n",
    "state_events = {0: [codec.encode_event(Event(\"tie\", 0))]}\n",
    "ds = NoteEncodingState()\n",
    "num_segments = np.ceil(time_stamps.max() / 512).astype(int)\n",
    "for time in time_stamps:\n",
    "    segment_num = time // 512\n",
    "    event_idx = np.nonzero(quantized_time == time)[0]\n",
    "    event_values = [values[i] for i in event_idx]\n",
    "    event = events.get(segment_num, [])\n",
    "    tokens = [codec.encode_event(token) for event in event_values for token in note_event_data_to_events(ds, event, codec)]\n",
    "    tokens.insert(0, time % 512)\n",
    "    event.extend(tokens)\n",
    "    events[segment_num] = event\n",
    "    state_event = [codec.encode_event(s) for s in note_encoding_state_to_events(ds)]\n",
    "    state_events[segment_num+1] = state_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = torch.zeros(len(events), 2048).int()\n",
    "for k, v in events.items():\n",
    "    all_events = state_events[k] + v\n",
    "    results[k][:len(all_events)] = torch.Tensor(all_events).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decode_events(\n",
    "    state: DS,\n",
    "    tokens: np.ndarray,\n",
    "    start_time: int,\n",
    "    max_time: Optional[int],\n",
    "    codec: Codec,\n",
    "    decode_event_fn: Callable[[DS, float, Event, Codec],\n",
    "                              None],\n",
    ") -> Tuple[int, int]:\n",
    "  \"\"\"Decode a series of tokens, maintaining a decoding state object.\n",
    "\n",
    "  Args:\n",
    "    state: Decoding state object; will be modified in-place.\n",
    "    tokens: event tokens to convert.\n",
    "    start_time: offset start time if decoding in the middle of a sequence.\n",
    "    max_time: Events at or beyond this time will be dropped.\n",
    "    codec: An event_codec.Codec object that maps indices to Event objects.\n",
    "    decode_event_fn: Function that consumes an Event (and the current time) and\n",
    "        updates the decoding state.\n",
    "\n",
    "  Returns:\n",
    "    invalid_events: number of events that could not be decoded.\n",
    "    dropped_events: number of events dropped due to max_time restriction.\n",
    "  \"\"\"\n",
    "  invalid_events = 0\n",
    "  dropped_events = 0\n",
    "  cur_steps = 0\n",
    "  cur_time = start_time\n",
    "  token_idx = 0\n",
    "  for token_idx, token in enumerate(tokens):\n",
    "    try:\n",
    "      event = codec.decode_event_index(token)\n",
    "    except ValueError:\n",
    "      invalid_events += 1\n",
    "      continue\n",
    "    if event.type == 'shift':\n",
    "      cur_steps += event.value\n",
    "      cur_time = start_time + cur_steps / codec.steps_per_second\n",
    "      if max_time and cur_time > max_time:\n",
    "        dropped_events = len(tokens) - token_idx\n",
    "        break\n",
    "    else:\n",
    "      cur_steps = 0\n",
    "      try:\n",
    "        decode_event_fn(state, cur_time, event, codec)\n",
    "      except ValueError:\n",
    "        invalid_events += 1\n",
    "        print(\n",
    "            f\"Got invalid event when decoding event {event} at time {cur_time}. Invalid event counter now at {invalid_events}.\")\n",
    "        continue\n",
    "  return invalid_events, dropped_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_note_to_sequence(\n",
    "    ns: note_seq.NoteSequence,\n",
    "    start_time: float, end_time: float, pitch: int, velocity: int,\n",
    "    program: int = 0, is_drum: bool = False\n",
    ") -> None:\n",
    "  end_time = max(end_time, start_time + 0.01)\n",
    "  ns.notes.add(\n",
    "      start_time=start_time, end_time=end_time,\n",
    "      pitch=pitch, velocity=velocity, program=program, is_drum=is_drum)\n",
    "  ns.total_time = max(ns.total_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class NoteDecodingState:\n",
    "  \"\"\"Decoding state for note transcription.\"\"\"\n",
    "  current_time: float = 0.0\n",
    "  # velocity to apply to subsequent pitch events (zero for note-off)\n",
    "  current_velocity: int = 100 \n",
    "  # program to apply to subsequent pitch events\n",
    "  current_program: int = 0\n",
    "  # onset time and velocity for active pitches and programs\n",
    "  active_pitches: MutableMapping[Tuple[int, int],\n",
    "                                 Tuple[float, int]] = dataclasses.field(\n",
    "                                     default_factory=dict)\n",
    "  # pitches (with programs) to continue from previous segment\n",
    "  tied_pitches: MutableSet[Tuple[int, int]] = dataclasses.field(\n",
    "      default_factory=set)\n",
    "  # whether or not we are in the tie section at the beginning of a segment\n",
    "  is_tie_section: bool = False\n",
    "  # partially-decoded NoteSequence\n",
    "  note_sequence: note_seq.NoteSequence = dataclasses.field(\n",
    "      default_factory=lambda: note_seq.NoteSequence(ticks_per_quarter=220))\n",
    "\n",
    "def decode_note_event(\n",
    "    state: NoteDecodingState,\n",
    "    time: float,\n",
    "    event: Event,\n",
    "    codec: Codec\n",
    ") -> None:\n",
    "  \"\"\"Process note event and update decoding state.\"\"\"\n",
    "  if time < state.current_time:\n",
    "    raise ValueError('event time < current time, %f < %f' % (\n",
    "        time, state.current_time))\n",
    "  state.current_time = time\n",
    "  if event.type == 'pitch':\n",
    "    pitch = event.value\n",
    "    if state.is_tie_section:\n",
    "      # \"tied\" pitch\n",
    "      if (pitch, state.current_program) not in state.active_pitches:\n",
    "        raise ValueError('inactive pitch/program in tie section: %d/%d' %\n",
    "                         (pitch, state.current_program))\n",
    "      if (pitch, state.current_program) in state.tied_pitches:\n",
    "        raise ValueError('pitch/program is already tied: %d/%d' %\n",
    "                         (pitch, state.current_program))\n",
    "      state.tied_pitches.add((pitch, state.current_program))\n",
    "    elif state.current_velocity == 0:\n",
    "      # note offset\n",
    "      if (pitch, state.current_program) not in state.active_pitches:\n",
    "        raise ValueError('note-off for inactive pitch/program: %d/%d' %\n",
    "                         (pitch, state.current_program))\n",
    "      onset_time, onset_velocity = state.active_pitches.pop(\n",
    "          (pitch, state.current_program))\n",
    "      _add_note_to_sequence(\n",
    "          state.note_sequence, start_time=onset_time, end_time=time,\n",
    "          pitch=pitch, velocity=onset_velocity, program=state.current_program)\n",
    "    else:\n",
    "      # note onset\n",
    "      if (pitch, state.current_program) in state.active_pitches:\n",
    "        # The pitch is already active; this shouldn't really happen but we'll\n",
    "        # try to handle it gracefully by ending the previous note and starting a\n",
    "        # new one.\n",
    "        onset_time, onset_velocity = state.active_pitches.pop(\n",
    "            (pitch, state.current_program))\n",
    "        _add_note_to_sequence(\n",
    "            state.note_sequence, start_time=onset_time, end_time=time,\n",
    "            pitch=pitch, velocity=onset_velocity, program=state.current_program)\n",
    "      state.active_pitches[(pitch, state.current_program)] = (\n",
    "          time, state.current_velocity)\n",
    "  elif event.type == 'drum':\n",
    "    # drum onset (drums have no offset)\n",
    "    if state.current_velocity == 0:\n",
    "      raise ValueError('velocity cannot be zero for drum event')\n",
    "    offset_time = time + 0.01\n",
    "    _add_note_to_sequence(\n",
    "        state.note_sequence, start_time=time, end_time=offset_time,\n",
    "        pitch=event.value, velocity=state.current_velocity, is_drum=True)\n",
    "  elif event.type == 'velocity':\n",
    "    # velocity change\n",
    "    num_velocity_bins = vocabularies.num_velocity_bins_from_codec(codec)\n",
    "    velocity = vocabularies.bin_to_velocity(event.value, num_velocity_bins)\n",
    "    state.current_velocity = velocity\n",
    "  elif event.type == 'program':\n",
    "    # program change\n",
    "    state.current_program = event.value\n",
    "  elif event.type == 'tie':\n",
    "    # end of tie section; end active notes that weren't declared tied\n",
    "    if not state.is_tie_section:\n",
    "      raise ValueError('tie section end event when not in tie section')\n",
    "    for (pitch, program) in list(state.active_pitches.keys()):\n",
    "      if (pitch, program) not in state.tied_pitches:\n",
    "        onset_time, onset_velocity = state.active_pitches.pop((pitch, program))\n",
    "        _add_note_to_sequence(\n",
    "            state.note_sequence,\n",
    "            start_time=onset_time, end_time=state.current_time,\n",
    "            pitch=pitch, velocity=onset_velocity, program=program)\n",
    "    state.is_tie_section = False\n",
    "  else:\n",
    "    raise ValueError('unexpected event type: %s' % event.type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got invalid event when decoding event %s at time %f. Invalid event counter now at %d. Event(type='tie', value=tensor(0.)) 0 1\n",
      "Got invalid event when decoding event %s at time %f. Invalid event counter now at %d. Event(type='pitch', value=tensor(68.)) tensor(2.3800) 2\n",
      "Got invalid event when decoding event %s at time %f. Invalid event counter now at %d. Event(type='pitch', value=tensor(59.)) tensor(2.3900) 3\n",
      "Got invalid event when decoding event %s at time %f. Invalid event counter now at %d. Event(type='pitch', value=tensor(64.)) tensor(2.3900) 4\n",
      "Got invalid event when decoding event %s at time %f. Invalid event counter now at %d. Event(type='pitch', value=tensor(64.)) tensor(2.8100) 5\n",
      "Got invalid event when decoding event %s at time %f. Invalid event counter now at %d. Event(type='pitch', value=tensor(68.)) tensor(2.8100) 6\n",
      "Got invalid event when decoding event %s at time %f. Invalid event counter now at %d. Event(type='pitch', value=tensor(59.)) tensor(2.8200) 7\n",
      "Got invalid event when decoding event %s at time %f. Invalid event counter now at %d. Event(type='pitch', value=tensor(68.)) tensor(3.2500) 8\n",
      "Got invalid event when decoding event %s at time %f. Invalid event counter now at %d. Event(type='pitch', value=tensor(59.)) tensor(3.2600) 9\n",
      "Got invalid event when decoding event %s at time %f. Invalid event counter now at %d. Event(type='pitch', value=tensor(64.)) tensor(3.2600) 10\n",
      "Got invalid event when decoding event %s at time %f. Invalid event counter now at %d. Event(type='pitch', value=tensor(64.)) tensor(3.6900) 11\n",
      "Got invalid event when decoding event %s at time %f. Invalid event counter now at %d. Event(type='pitch', value=tensor(68.)) tensor(3.7000) 12\n",
      "Got invalid event when decoding event %s at time %f. Invalid event counter now at %d. Event(type='pitch', value=tensor(59.)) tensor(3.7200) 13\n",
      "Got invalid event when decoding event %s at time %f. Invalid event counter now at %d. Event(type='pitch', value=tensor(64.)) tensor(4.3500) 14\n",
      "Got invalid event when decoding event %s at time %f. Invalid event counter now at %d. Event(type='pitch', value=tensor(59.)) tensor(4.3700) 15\n",
      "Got invalid event when decoding event %s at time %f. Invalid event counter now at %d. Event(type='pitch', value=tensor(68.)) tensor(4.3700) 16\n",
      "Got invalid event when decoding event %s at time %f. Invalid event counter now at %d. Event(type='pitch', value=tensor(40.)) tensor(4.4600) 17\n",
      "Got invalid event when decoding event %s at time %f. Invalid event counter now at %d. Event(type='pitch', value=tensor(52.)) tensor(4.4600) 18\n",
      "Got invalid event when decoding event %s at time %f. Invalid event counter now at %d. Event(type='pitch', value=tensor(68.)) tensor(4.6600) 19\n",
      "Got invalid event when decoding event %s at time %f. Invalid event counter now at %d. Event(type='pitch', value=tensor(64.)) tensor(4.7100) 20\n",
      "Got invalid event when decoding event %s at time %f. Invalid event counter now at %d. Event(type='pitch', value=tensor(59.)) tensor(4.7200) 21\n"
     ]
    }
   ],
   "source": [
    "decoding_state = NoteDecodingState()\n",
    "invalid_ids, dropped_events = decode_events(\n",
    "    state=decoding_state, tokens=results[0], start_time=0, max_time=None,\n",
    "    codec=codec, decode_event_fn=decode_note_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "644, 572, 644, 577, 644, 580, 644, 584, 644, 589, 644, 592, 644, 596, 643"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "(events_, event_start_indices, event_end_indices, state_events_, state_event_indices, ) = encode_and_index_events(\n",
    "    NoteEncodingState(), times, values, note_event_data_to_events, codec, frame_times, note_encoding_state_to_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_start_idx, seg_end_idx, seg_state_idx, segment_times = split_tokens(\n",
    "    [event_start_indices, event_end_indices, state_event_indices, frame_times],\n",
    "    segment_length=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([643, 644, 565,  ..., 644, 553, 643], dtype=torch.int32)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_events_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([644, 572, 644, 577, 643, 644, 553, 644, 572, 644, 577, 643, 644, 553,\n",
       "        644, 577, 643, 644, 553, 643], dtype=torch.int32)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_events_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([104416, 104416, 104416, 104416, 104416, 104416, 104416, 104416, 104416,\n",
       "        104416, 104416, 104416, 104416, 104416, 104440, 104440, 104476, 104489,\n",
       "        104489, 104489, 104489, 104489, 104489, 104489, 104489, 104489, 104489,\n",
       "        104489, 104489, 104489, 104489, 104489, 104489, 104489, 104489, 104489,\n",
       "        104489, 104489, 104489, 104489, 104489, 104489, 104489, 104489, 104489,\n",
       "        104489, 104652, 104757, 104772, 104772, 104772, 104772, 104772, 104772,\n",
       "        104772, 104772, 104772, 104772, 104772, 104772, 104772, 104772, 104772,\n",
       "        104772, 104772, 104772, 104772, 104772, 104772, 104772, 104772, 104772,\n",
       "        104772, 104772, 104772, 104772, 104772, 104772, 104772, 104772, 104772,\n",
       "        104772, 104772, 104772, 104772, 104772, 104772, 104852, 104852, 104852,\n",
       "        104852, 104852, 104852, 104852, 104852, 104877, 104901, 104901, 104961,\n",
       "        104964, 104964, 104973, 105013, 105013, 105013, 105028, 105028, 105028,\n",
       "        105028, 105028, 105028, 105028, 105028, 105028, 105028, 105028, 105028,\n",
       "        105028, 105028, 105028, 105028, 105028, 105028, 105028, 105028, 105028,\n",
       "        105028, 105028, 105028, 105028, 105028, 105028, 105028, 105028, 105028,\n",
       "        105028, 105028, 105028, 105028, 105028, 105028, 105028, 105028, 105028,\n",
       "        105108, 105108, 105108, 105108, 105108, 105108, 105108, 105108, 105108,\n",
       "        105108, 105108, 105133, 105144, 105144, 105144, 105177, 105192, 105192,\n",
       "        105192, 105208, 105228, 105228, 105228, 105228, 105228, 105228, 105228,\n",
       "        105228, 105228, 105228, 105228, 105228, 105228, 105228, 105228, 105228,\n",
       "        105228, 105228, 105228, 105228, 105228, 105228, 105228, 105228, 105228,\n",
       "        105228, 105228, 105228, 105228, 105228, 105228, 105228, 105228, 105228,\n",
       "        105228, 105228, 105228, 105228, 105228, 105228, 105228, 105228, 105228,\n",
       "        105228, 105228, 105228, 105228, 105228, 105228, 105228, 105228, 105228,\n",
       "        105228, 105228, 105228, 105228, 105241, 105268, 105268, 105268, 105268,\n",
       "        105268, 105268, 105268, 105268, 105268, 105268, 105268, 105268, 105268,\n",
       "        105268, 105268, 105268, 105268, 105268, 105268, 105268, 105268, 105268,\n",
       "        105268, 105268, 105268, 105268, 105268, 105268, 105268, 105268, 105268,\n",
       "        105268, 105268, 105268, 105268], dtype=torch.int32)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_state_idx[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([644., 548., 644., 560., 644., 571., 643.,   3., 644., 641., 571.,   4.,\n",
       "        644., 641., 548.,   5., 644., 642., 572.,  15., 644., 642., 573.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = 59\n",
    "c = extract_sequence_with_indices(events_, seg_start_idx[id, 0], seg_end_idx[id, -1], \n",
    "        torch.Tensor(state_events_), codec.encode_event(Event(\"tie\", 0)), seg_state_idx[id])[:40]\n",
    "count_shift_and_pad(c, 200, codec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "550 in c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   2,   4,   6,   8,  10,  12,  14,  16,  18,  20,  22,  24,  26,\n",
       "         28,  30,  32,  34,  36,  38,  40,  42,  44,  46,  48,  50,  52,  54,\n",
       "         56,  58,  60,  62,  64,  66,  68,  70,  72,  74,  76,  78,  80,  82,\n",
       "         84,  86,  88,  90,  92,  94,  96,  98, 103, 108, 110, 112, 114, 116,\n",
       "        118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144,\n",
       "        146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172,\n",
       "        174, 176, 178, 180, 188, 193, 195, 197, 199, 201, 203, 205, 207, 209,\n",
       "        211, 213, 215, 217, 219, 221, 223, 225, 227, 229, 231, 233, 235, 237,\n",
       "        239, 241, 243, 245, 247, 249, 251, 253, 273, 275, 277, 279, 281, 283,\n",
       "        285, 287, 289, 291, 293, 295, 297, 299, 301, 303, 305, 307, 309, 311,\n",
       "        313, 327, 335, 337, 339, 341, 343, 345, 347, 349, 351, 353, 355, 357,\n",
       "        359, 361, 363, 365, 367, 369, 371, 373, 375, 383, 397, 399, 401, 403,\n",
       "        405, 407, 409, 411, 413, 415, 417, 419, 421, 423, 425, 427, 429, 431,\n",
       "        433, 435, 437, 445, 453, 461, 463, 465, 467, 469, 471, 473, 475, 477,\n",
       "        479, 481, 483, 485, 487, 489, 491, 493, 495, 497, 499, 501, 503, 505,\n",
       "        507, 509, 511, 513, 515, 517, 519, 530, 538, 552, 554, 556, 558, 560,\n",
       "        568, 570, 572, 574, 576, 578, 580, 582, 584, 586, 591, 593, 598, 603,\n",
       "        605, 607, 609, 611, 613, 615, 617, 619, 621, 623, 625, 627, 629, 631,\n",
       "        636, 644, 646, 648], dtype=torch.int32)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_start_idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([644, 548, 644, 560, 644, 567, 644, 572, 644, 577, 643,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1, 644, 642, 562,\n",
       "        644, 642, 571, 644, 642, 550,   1,   1,   1,   1, 644, 641, 548, 644,\n",
       "        641, 560, 644, 641, 567, 644, 641, 572, 644, 641, 577,   1, 644, 642,\n",
       "        567,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1, 644, 641,\n",
       "        567,   1,   1,   1, 644, 641, 571,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1, 644, 642, 571,\n",
       "          1,   1,   1,   1,   1, 644, 642, 567,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1, 644, 642, 560,   1,\n",
       "        644, 642, 548,   1,   1, 644, 642, 570,   1,   1, 644, 642, 572,   1,\n",
       "          1,   1,   1,   1, 644, 641, 550, 644, 641, 562, 644, 641, 567, 644,\n",
       "        641, 571,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1, 644, 641, 572,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1, 644, 641,\n",
       "        570, 644, 642, 570,   1, 644, 642, 572,   1,   1,   1, 644, 642, 579,\n",
       "        644, 642, 577,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1, 644, 642, 582,   1,   1,   1, 644, 641, 548, 644, 641, 560, 644,\n",
       "        641, 570, 644, 641, 572, 644, 641, 577, 644, 641, 579, 644, 642, 572,\n",
       "        644, 642, 548,   1, 644, 642, 560,   1,   1, 644, 642, 576,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1, 644, 641, 576,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1, 644, 642, 576,   1,\n",
       "          1,   1, 644, 642, 570,   1,   1, 644, 641, 572, 644, 642, 572,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1, 644, 642, 553,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1, 644, 641, 548, 644, 641, 560, 644, 641, 570,\n",
       "        644, 641, 582,   1,   1,   1, 644, 641, 572,   1,   1,   1, 644, 641,\n",
       "        576,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1, 644, 642, 577,   1, 644, 642, 569,\n",
       "        644, 642, 572,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1, 644, 641,\n",
       "        577, 644, 642, 577,   1, 644, 641, 572, 644, 642, 572, 644, 641, 569,\n",
       "        644, 642, 569,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tie_end_token = codec.encode_event(Event(\"tie\", 0))\n",
    "extract_sequence_with_indices(events, seg_end_idx[2, 0], seg_end_idx[2, -1], \n",
    "        torch.Tensor(state_events), tie_end_token, seg_state_idx[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1,   1, 644,  ..., 596,   1,   1], dtype=torch.int32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_result = count_shift_and_pad(event_seg, 2048, codec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_result = count_shift_and_pad_torch(event_seg, 2048, codec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(original_result == torch_result).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca9c10e22fb59a4147419ecfb4f4ace8bf6cad437df55de8cfa3d4a753e2242b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
